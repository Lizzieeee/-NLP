{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京意淫到了脑残的地步，看了恶心想吐', '首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹', '吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋律，为了煽情而煽情，让人觉得他是个大做作、大谎言家。（7.29更新）片子整体不如湄公河行动，1.整体不够流畅，编剧有毒，台词尴尬；2.刻意做作的主旋律煽情显得如此不合时宜而又多余。', '凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。', '中二得很', '“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。', '脑子是个好东西，希望编剧们都能有。', '三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心，他终于抛弃李忠志了，新增外来班底让硬件实力有机会和国际接轨，开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶，在理念上，它甚至做到《绣春刀2》最想做到的那部分。', '开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹簧床架挡炸弹 空手接碎玻璃 弹匣割喉等帅得飞起！就算前半段铺垫节奏散漫主角光环开太大等也不怕 作为一个中国人 两个小时弥漫着中国强大得不可侵犯的氛围 还是让那颗民族自豪心砰砰砰跳个不停。', '15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个人，无能的政府需要求助于这些英雄才能解决难题，体现的是个人的价值，所以主旋律照抄这种模式实际上是有问题的。我们以前嘲笑个人英雄主义，却没想到捆绑爱国主义的全能战士更加难以下咽。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('D:\\\\NLP\\Assignment\\\\L1\\\\movie_comments.csv',encoding='UTF-8')\n",
    "comments = file['comment'].tolist()\n",
    "print(comments[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吴京意淫到了脑残的地步看了恶心\n"
     ]
    }
   ],
   "source": [
    "import re,string\n",
    "from zhon import hanzi\n",
    "def token(stri):\n",
    "    punc = string.punctuation + hanzi.punctuation\n",
    "    t = re.findall('\\w+',str(stri))\n",
    "    return ''.join(a for a in t if a not in punc)    \n",
    "comments_clean = token(comments)\n",
    "print(comments_clean[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\canaan\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.621 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心']\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "comments_words = cut(comments_clean)\n",
    "print(comments_words[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "意淫到\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(comments_words[1]+comments_words[2])\n",
    "print(type(comments_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328256),\n",
       " ('了', 102409),\n",
       " ('是', 73421),\n",
       " ('我', 50510),\n",
       " ('都', 36251),\n",
       " ('很', 34755),\n",
       " ('看', 33853),\n",
       " ('电影', 33638),\n",
       " ('也', 32064),\n",
       " ('和', 31291)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words_count = Counter(comments_words)\n",
    "words_count.most_common(10)\n",
    "#frequencies = [f for f in words_count.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017941751635849222"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = comments_words\n",
    "def prob_1(word):\n",
    "    return words_count[word]/len(token)\n",
    "prob_1('好看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_2(token):\n",
    "    token_2 = []\n",
    "    for i in range(len(token[:-1])):\n",
    "        t = token[i]+token[i+1]\n",
    "        token_2.append(t)\n",
    "    return token_2\n",
    "token_2 = token_2(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['感觉就是', '就是他', '他男朋友', '男朋友真让人', '真让人恶心']\n"
     ]
    }
   ],
   "source": [
    "print(token_2[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009254693849385005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_2 = Counter(token_2)\n",
    "def prob_2(word1,word2):\n",
    "    if word1+word2 in words_count_2:\n",
    "        return words_count_2[word1+word2]/len(token_2)\n",
    "    else:\n",
    "        return (prob_1(word1)+prob_1(word2))/2\n",
    "prob_2('晚上','好看')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.902905409002553e-46\n",
      "1.5344059099420902e-46\n",
      "2.4673269932002946e-30\n",
      "2.061900138767372e-32\n"
     ]
    }
   ],
   "source": [
    "def sentence_prob(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_prob=1\n",
    "    for i,w in enumerate(sentence[:-1]):\n",
    "        next_2 = sentence[i+1]\n",
    "        prob = prob_2(w,next_2)\n",
    "        sentence_prob *=prob\n",
    "    return sentence_prob\n",
    "print(sentence_prob('电影又精彩又令人感到兴奋'))\n",
    "print(sentence_prob('电影又精彩又令人感到无聊'))\n",
    "print(sentence_prob('电影又精彩又好看'))\n",
    "print(sentence_prob('电影又精彩又无聊'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_grammer(gram,split):\n",
    "    grammer = {}\n",
    "    for line in gram.split('\\n'):\n",
    "        if not line.strip(): continue\n",
    "        exp,stmt = line.split(split)\n",
    "        grammer[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammer\n",
    "\n",
    "def generate(gram,target):\n",
    "    if target in gram:\n",
    "        expand_next = random.choice(gram[target])\n",
    "        expanded_next = [generate(gram,t) for t in expand_next]\n",
    "        return ''.join([e for e in expanded_next])        \n",
    "    else:\n",
    "        return target\n",
    "    \n",
    "def generate_n(gram,target,n):\n",
    "    g_n = []\n",
    "    for i in range(n):\n",
    "        if target in gram:\n",
    "            expand_next = random.choice(gram[target])\n",
    "            expanded_next = [generate(gram,t) for t in expand_next]\n",
    "            t = ''.join([e for e in expanded_next]) \n",
    "        else:\n",
    "            t = target\n",
    "        g_n.append(str(t))\n",
    "    return g_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我找找乐子', '俺看看乐子', '我想找点玩的', '俺想找点玩的', '我们想找点乐子']\n"
     ]
    }
   ],
   "source": [
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 =    我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "grammer_human = create_grammer(human,'=')\n",
    "grammer_host = create_grammer(host,'=')\n",
    "generate_human = generate(grammer_human,'human')\n",
    "generate_host = generate(grammer_host,'host')\n",
    "generate_n_human = generate_n(grammer_human,'human',5)\n",
    "print(generate_n_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "先生,你好我是1号,请问你要玩一玩赌博吗？\n"
     ]
    }
   ],
   "source": [
    "def generate_best(gram,target,n):\n",
    "    sentences = generate_n(gram,target,n)\n",
    "    sentences_prob = []\n",
    "    for s in sentences:\n",
    "        sentences_prob.append(sentence_prob(s))\n",
    "    sentences_prob = list(enumerate(sentences_prob))\n",
    "    sentences_prob.sort(key=lambda x:x[1],reverse=True)\n",
    "    index = sentences_prob[0][0]\n",
    "    return sentences[index]\n",
    "print(generate_best(grammer_host,'host',5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？\n",
    "Ans: 尝试后发现由2-Gram model得出概率的准确性与句子长度呈负相关关系。可以用长的马尔可夫链模型尝试改进。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
